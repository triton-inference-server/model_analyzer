# Copyright (c) 2022, NVIDIA CORPORATION & AFFILIATES. All rights reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

from typing import Dict, List, Union, Optional, Generator

from .config_generator_interface import ConfigGeneratorInterface

from model_analyzer.config.generate.base_model_config_generator import BaseModelConfigGenerator
from model_analyzer.config.generate.search_config import SearchConfig
from model_analyzer.config.generate.coordinate import Coordinate
from model_analyzer.config.generate.coordinate_data import CoordinateData
from model_analyzer.config.generate.neighborhood import Neighborhood
from model_analyzer.config.generate.brute_run_config_generator import BruteRunConfigGenerator
from model_analyzer.config.generate.model_variant_name_manager import ModelVariantNameManager
from model_analyzer.config.generate.model_profile_spec import ModelProfileSpec
from model_analyzer.config.run.model_run_config import ModelRunConfig
from model_analyzer.config.run.run_config import RunConfig
from model_analyzer.perf_analyzer.perf_config import PerfAnalyzerConfig
from model_analyzer.triton.model.model_config import ModelConfig
from model_analyzer.triton.client.client import TritonClient
from model_analyzer.device.gpu_device import GPUDevice
from model_analyzer.config.input.config_command_profile import ConfigCommandProfile
from model_analyzer.result.run_config_measurement import RunConfigMeasurement

from model_analyzer.constants import LOGGER_NAME

import logging

logger = logging.getLogger(LOGGER_NAME)


class QuickRunConfigGenerator(ConfigGeneratorInterface):
    """
    Hill climbing algorithm to create RunConfigs
    """

    def __init__(self, search_config: SearchConfig,
                 config: ConfigCommandProfile, gpus: List[GPUDevice],
                 models: List[ModelProfileSpec], client: TritonClient,
                 model_variant_name_manager: ModelVariantNameManager):
        """
        Parameters
        ----------
        search_config: SearchConfig
            Defines parameters and dimensions for the search
        config: ConfigCommandProfile
            Profile configuration information
        gpus: List of GPUDevices
        models: List of ConfigModelProfileSpec
            List of models to profile
        client: TritonClient
        model_variant_name_manager: ModelVariantNameManager
        """
        self._search_config = search_config
        self._config = config
        self._models = models
        self._client = client
        self._model_variant_name_manager = model_variant_name_manager

        self._triton_env = BruteRunConfigGenerator.determine_triton_server_env(
            models)

        # This tracks measured results for all coordinates
        self._coordinate_data = CoordinateData()

        # This is an initial center that the neighborhood is built around.
        # It is updated every new creation of the neighborhood.
        self._home_coordinate = self._get_starting_coordinate()

        # This is the coordinate that we want to measure next. It is
        # updated every step of this generator
        self._coordinate_to_measure: Coordinate = self._home_coordinate

        # Track the best coordinate seen so far that can be used during
        # the back-off stage.
        self._best_coordinate = self._home_coordinate
        self._best_measurement: Optional[RunConfigMeasurement] = None

        self._neighborhood = Neighborhood(
            self._search_config.get_neighborhood_config(),
            self._home_coordinate, self._coordinate_data)

        # Sticky bit. Once true, we should never stay at a home that is failing or None
        self._home_has_passed = False

        self._done = False

    def _is_done(self) -> bool:
        return self._done

    def get_configs(self) -> Generator[RunConfig, None, None]:
        """
        Returns
        -------
        RunConfig
            The next RunConfig generated by this class
        """
        config = self._create_default_run_config()
        yield (config)

        while True:
            if self._is_done():
                break

            config = self._get_next_run_config()
            yield (config)
            self._step()

    def _step(self) -> None:
        """
        Determine self._coordinate_to_measure, which is what is used to
        create the next RunConfig
        """
        if self._should_step_back():
            self._take_step_back()
        elif self._neighborhood.enough_coordinates_initialized():
            self._take_step()
        else:
            self._pick_coordinate_to_initialize()

    def set_last_results(
            self, measurements: List[Optional[RunConfigMeasurement]]) -> None:
        """
        Given the results from the last RunConfig, make decisions
        about future configurations to generate

        Parameters
        ----------
        measurements: List of Measurements from the last run(s)
        """
        self._coordinate_data.set_measurement(
            coordinate=self._coordinate_to_measure, measurement=measurements[0])

        if measurements[0] is not None:
            self._update_best_measurement(measurement=measurements[0])

            if self._measuring_home_coordinate(
            ) and measurements[0].is_passing_constraints():
                self._home_has_passed = True

        self._print_debug_logs(measurements)

    def _update_best_measurement(self,
                                 measurement: RunConfigMeasurement) -> None:
        """Keep track of the best coordinate/measurement seen so far."""
        if self._best_measurement is None:
            self._best_coordinate = self._coordinate_to_measure
            self._best_measurement = measurement

        elif not self._best_measurement.is_passing_constraints(
        ) and measurement.is_passing_constraints():
            self._best_coordinate = self._coordinate_to_measure
            self._best_measurement = measurement

        elif not self._best_measurement.is_passing_constraints(
        ) and not measurement.is_passing_constraints():
            comparison = self._best_measurement.compare_constraints(
                other=measurement)

            if comparison and comparison > 0:
                self._best_coordinate = self._coordinate_to_measure
                self._best_measurement = measurement

        elif self._best_measurement.is_passing_constraints(
        ) and measurement.is_passing_constraints():
            comparison = self._best_measurement.compare_measurements(
                other=measurement)

            if comparison and comparison > 0:
                self._best_coordinate = self._coordinate_to_measure
                self._best_measurement = measurement

    def _get_last_results(self) -> Optional[RunConfigMeasurement]:
        return self._coordinate_data.get_measurement(
            coordinate=self._coordinate_to_measure)

    def _take_step(self) -> None:
        new_coordinate = self._neighborhood.determine_new_home()
        self._determine_if_done(new_coordinate)

        logger.debug(f"Stepping {self._home_coordinate}->{new_coordinate}")
        self._home_coordinate = new_coordinate
        self._coordinate_to_measure = new_coordinate
        self._recreate_neighborhood(force_slow_mode=False)

    def _take_step_back(self) -> None:
        new_coordinate = self._neighborhood.get_nearest_neighbor(
            coordinate_in=self._best_coordinate)

        # TODO: TMA-871: handle back-off (and its termination) better.
        if new_coordinate == self._home_coordinate:
            self._done = True

        logger.debug(
            f"Stepping back: {self._home_coordinate}->{new_coordinate}")
        self._home_coordinate = new_coordinate
        self._coordinate_to_measure = new_coordinate
        self._recreate_neighborhood(force_slow_mode=True)

    def _should_step_back(self) -> bool:
        """
        Step back if take any of the following steps:
          - Step from a passing home to a failing home
          - Step from any home to home with a None measurement
        """
        if self._measuring_home_coordinate():
            last_results = self._get_last_results()
            if not last_results:
                return True
            last_results_passed = last_results.is_passing_constraints()
            if not last_results_passed and self._home_has_passed:
                return True
        return False

    def _measuring_home_coordinate(self) -> bool:
        return self._coordinate_to_measure == self._home_coordinate

    def _determine_if_done(self, new_coordinate: Coordinate) -> None:
        """
        Based on the new coordinate picked, determine if the generator is done
        and if so, update self._done
        """
        if new_coordinate == self._home_coordinate:
            self._done = True
        if self._coordinate_data.get_visit_count(new_coordinate) >= 2:
            self._done = True

    def _recreate_neighborhood(self, force_slow_mode: bool) -> None:
        neighborhood_config = self._search_config.get_neighborhood_config()

        self._neighborhood = Neighborhood(neighborhood_config,
                                          self._home_coordinate,
                                          self._coordinate_data)

        self._coordinate_data.increment_visit_count(self._home_coordinate)

        if force_slow_mode:
            self._neighborhood.force_slow_mode()

    def _pick_coordinate_to_initialize(self) -> None:
        next_coordinate = self._neighborhood.pick_coordinate_to_initialize()

        if next_coordinate:
            self._coordinate_to_measure = next_coordinate
            logger.debug(
                f"Need more data. Measuring {self._coordinate_to_measure}")
        else:
            logger.info("No coordinate to measure. Exiting")
            self._done = True

    def _get_starting_coordinate(self) -> Coordinate:
        min_indexes = self._search_config.get_min_indexes()
        return Coordinate(min_indexes)

    def _get_coordinate_values(self, coordinate: Coordinate,
                               key: int) -> Dict[str, Union[int, float]]:
        dims = self._search_config.get_dimensions()
        values = dims.get_values_for_coordinate(coordinate)
        return values[key]

    def _get_next_run_config(self) -> RunConfig:
        run_config = RunConfig(self._triton_env)

        for i, _ in enumerate(self._models):
            mrc = self._get_next_model_run_config(i)
            run_config.add_model_run_config(mrc)

        return run_config

    def _get_next_model_run_config(self, model_num: int) -> ModelRunConfig:
        mc = self._get_next_model_config(model_num)

        model_variant_name = mc.get_field('name')
        pac = self._get_next_perf_analyzer_config(model_variant_name, model_num)

        model_name = self._models[model_num].model_name()
        return ModelRunConfig(model_name, mc, pac)

    def _get_next_model_config(self, model_num: int) -> ModelConfig:
        dimension_values = self._get_coordinate_values(
            self._coordinate_to_measure, model_num)

        kind = "KIND_CPU" if self._models[model_num].cpu_only() else "KIND_GPU"
        param_combo: dict = {
            'instance_group': [{
                'count': dimension_values['instance_count'],
                'kind': kind,
            }]
        }

        if 'max_batch_size' in dimension_values:
            param_combo['max_batch_size'] = dimension_values['max_batch_size']

        if self._models[model_num].supports_dynamic_batching():
            param_combo['dynamic_batching'] = {}

        model_config = BaseModelConfigGenerator.make_model_config(
            param_combo=param_combo,
            model=self._models[model_num],
            model_variant_name_manager=self._model_variant_name_manager)
        return model_config

    def _get_next_perf_analyzer_config(self, model_variant_name: str,
                                       model_num: int) -> PerfAnalyzerConfig:
        dimension_values = self._get_coordinate_values(
            self._coordinate_to_measure, model_num)

        perf_analyzer_config = PerfAnalyzerConfig()

        perf_analyzer_config.update_config_from_profile_config(
            model_variant_name, self._config)

        model_batch_size = dimension_values.get("max_batch_size", 1)
        instance_count = dimension_values.get("instance_count", 1)

        concurrency = 2 * model_batch_size * instance_count

        perf_config_params = {'batch-size': 1, 'concurrency-range': concurrency}
        perf_analyzer_config.update_config(perf_config_params)

        perf_analyzer_config.update_config(
            self._models[model_num].perf_analyzer_flags())
        return perf_analyzer_config

    def _create_default_run_config(self) -> RunConfig:
        default_run_config = RunConfig(self._triton_env)

        for model in self._models:
            default_model_config = BaseModelConfigGenerator.make_model_config(
                param_combo={},
                model=model,
                model_variant_name_manager=self._model_variant_name_manager)

            default_perf_analyzer_config = PerfAnalyzerConfig()
            default_perf_analyzer_config.update_config_from_profile_config(
                default_model_config.get_field('name'), self._config)

            perf_config_params = {'batch-size': 1, 'concurrency-range': 1}
            default_perf_analyzer_config.update_config(perf_config_params)

            default_model_run_config = ModelRunConfig(
                model.model_name(), default_model_config,
                default_perf_analyzer_config)

            default_run_config.add_model_run_config(default_model_run_config)

        return default_run_config

    def _print_debug_logs(
            self, measurements: List[Union[RunConfigMeasurement,
                                           None]]) -> None:
        if measurements is not None and measurements[0] is not None:
            assert len(measurements) == 1

            throughput = measurements[0].get_non_gpu_metric_value(
                "perf_throughput")
            latency = measurements[0].get_non_gpu_metric_value(
                "perf_latency_p99")

            if self._best_measurement:
                best_throughput = self._best_measurement.get_non_gpu_metric_value(
                    "perf_throughput")
                best_latency = self._best_measurement.get_non_gpu_metric_value(
                    "perf_latency_p99")
            else:
                best_throughput = 0
                best_latency = 0

            logger.debug(
                f"Measurement for {self._coordinate_to_measure}: "
                f"throughput = {throughput}, latency = {latency} "
                f"(best throughput: {best_throughput}, best_latency: {best_latency})"
            )
        else:
            logger.debug(
                f"Measurement for {self._coordinate_to_measure}: None.")
