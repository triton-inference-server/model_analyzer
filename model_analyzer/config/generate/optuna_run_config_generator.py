#!/usr/bin/env python3

# Copyright 2022-2023, NVIDIA CORPORATION & AFFILIATES. All rights reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

import logging
from sys import maxsize
from typing import Any, Dict, Generator, List, Optional, Tuple, Union

import optuna

from model_analyzer.config.generate.base_model_config_generator import (
    BaseModelConfigGenerator,
)
from model_analyzer.config.generate.brute_run_config_generator import (
    BruteRunConfigGenerator,
)
from model_analyzer.config.generate.coordinate import Coordinate
from model_analyzer.config.generate.coordinate_data import CoordinateData
from model_analyzer.config.generate.model_profile_spec import ModelProfileSpec
from model_analyzer.config.generate.model_variant_name_manager import (
    ModelVariantNameManager,
)
from model_analyzer.config.generate.search_parameters import SearchParameters
from model_analyzer.config.input.config_command_profile import ConfigCommandProfile
from model_analyzer.config.input.config_defaults import DEFAULT_BATCH_SIZES
from model_analyzer.config.run.model_run_config import ModelRunConfig
from model_analyzer.config.run.run_config import RunConfig
from model_analyzer.constants import LOGGER_NAME
from model_analyzer.device.gpu_device import GPUDevice
from model_analyzer.perf_analyzer.perf_config import PerfAnalyzerConfig
from model_analyzer.record.metrics_manager import MetricsManager
from model_analyzer.result.run_config_measurement import RunConfigMeasurement
from model_analyzer.triton.client.client import TritonClient
from model_analyzer.triton.model.model_config import ModelConfig
from model_analyzer.triton.model.model_config_variant import ModelConfigVariant

from .config_generator_interface import ConfigGeneratorInterface
from .generator_utils import GeneratorUtils

logger = logging.getLogger(LOGGER_NAME)
from copy import deepcopy


class OptunaRunConfigGenerator(ConfigGeneratorInterface):
    """
    Hill climbing algorithm to create RunConfigs
    """

    def __init__(
        self,
        config: ConfigCommandProfile,
        gpus: List[GPUDevice],
        models: List[ModelProfileSpec],
        client: TritonClient,
        model_variant_name_manager: ModelVariantNameManager,
        search_parameters: SearchParameters,
        metrics_manager: MetricsManager,
    ):
        """
        Parameters
        ----------
        config: ConfigCommandProfile
            Profile configuration information
        gpus: List of GPUDevices
        models: List of ModelProfileSpec
            List of models to profile
        client: TritonClient
        model_variant_name_manager: ModelVariantNameManager
        search_parameters: SearchParameters
            The object that handles the users configuration search parameters
        """
        self._config = config
        self._client = client
        self._gpus = gpus
        self._models = models
        self._search_parameters = search_parameters
        self._metrics_manager = metrics_manager

        self._model_variant_name_manager = model_variant_name_manager

        self._triton_env = BruteRunConfigGenerator.determine_triton_server_env(models)

        self._num_models = len(models)
        self._curr_results: List = [[] for n in range(self._num_models)]

        self._c_api_mode = config.triton_launch_mode == "c_api"

        self._done = False

        self._sampler = optuna.samplers.TPESampler()
        self._study = optuna.create_study(
            study_name=self._models[0].model_name(),
            direction="maximize",
            sampler=self._sampler,
        )

    def _is_done(self) -> bool:
        return self._done

    def set_last_results(
        self, measurements: List[Optional[RunConfigMeasurement]]
    ) -> None:
        for index in range(self._num_models):
            self._curr_results[index].extend(measurements)

    def get_configs(self) -> Generator[RunConfig, None, None]:
        """
        Returns
        -------
        RunConfig
            The next RunConfig generated by this class
        """
        self._run_default_config()

        yield from self._study.optimize(self._objective, n_trials=50, timeout=600)

    def _objective(self, trial) -> float:
        instance_count = trial.suggest_int("instance_count", 1, 8)
        batch_size = int(2 ** trial.suggest_int("batch_size", 1, 10))
        concurrency = int(2 ** trial.suggest_int("concurrency", 1, 10))

        param_combo: Dict[str, Any] = {}
        param_combo["dynamic_batching"] = {}

        kind = "KIND_CPU" if self._models[0].cpu_only() else "KIND_GPU"
        param_combo["instance_group"] = [
            {
                "count": instance_count,
                "kind": kind,
            }
        ]

        param_combo["max_batch_size"] = batch_size

        model_config_variant = BaseModelConfigGenerator.make_model_config_variant(
            param_combo=param_combo,
            model=self._models[0],
            model_variant_name_manager=self._model_variant_name_manager,
            c_api_mode=self._c_api_mode,
        )

        run_config = RunConfig(self._triton_env)

        model_run_config = self._create_model_run_config(
            model=self._models[0],
            model_config_variant=model_config_variant,
            concurrency=concurrency,
        )

        run_config.add_model_run_config(model_run_config=model_run_config)

        # We should yield here!

        if run_config.is_legal_combination():
            measurement = self._metrics_manager.execute_run_config(run_config)

        if measurement:
            objectives = [model.objectives() for model in self._models]
            weightings = [model.weighting() for model in self._models]

            measurement.set_metric_weightings(metric_objectives=objectives)
            # measurement.set_constraint_manager(
            #     constraint_manager=self._constraint_manager
            # )
            measurement.set_model_config_weighting(model_config_weights=weightings)

            self.set_last_results([measurement])

            score = self._default_measurement.compare_measurements(measurement)  # type: ignore

            return score
        else:
            return -1

    def _run_default_config(self) -> None:
        default_run_config = RunConfig(self._triton_env)
        default_model_run_config = self._create_default_model_run_config(
            self._models[0]
        )
        default_run_config.add_model_run_config(default_model_run_config)

        if default_run_config.is_legal_combination():
            measurement = self._metrics_manager.execute_run_config(default_run_config)

        if measurement:
            objectives = [model.objectives() for model in self._models]
            weightings = [model.weighting() for model in self._models]

            measurement.set_metric_weightings(metric_objectives=objectives)
            # measurement.set_constraint_manager(
            #     constraint_manager=self._constraint_manager
            # )
            measurement.set_model_config_weighting(model_config_weights=weightings)

        self._default_measurement = measurement

    def _create_default_model_run_config(
        self, model: ModelProfileSpec
    ) -> ModelRunConfig:
        default_model_config_variant = (
            BaseModelConfigGenerator.make_model_config_variant(
                param_combo={},
                model=model,
                model_variant_name_manager=self._model_variant_name_manager,
                c_api_mode=self._c_api_mode,
            )
        )

        default_perf_analyzer_config = self._create_default_perf_analyzer_config(
            model, default_model_config_variant.model_config
        )

        default_model_run_config = ModelRunConfig(
            model.model_name(),
            default_model_config_variant,
            default_perf_analyzer_config,
        )

        return default_model_run_config

    def _create_default_perf_analyzer_config(
        self, model: ModelProfileSpec, model_config: ModelConfig
    ) -> PerfAnalyzerConfig:
        default_perf_analyzer_config = PerfAnalyzerConfig()
        default_perf_analyzer_config.update_config_from_profile_config(
            model_config.get_field("name"), self._config
        )

        default_concurrency = self._calculate_default_concurrency(model_config)

        perf_config_params = {
            "batch-size": DEFAULT_BATCH_SIZES,
            "concurrency-range": default_concurrency,
        }
        default_perf_analyzer_config.update_config(perf_config_params)

        default_perf_analyzer_config.update_config(model.perf_analyzer_flags())

        return default_perf_analyzer_config

    def _calculate_default_concurrency(self, model_config: ModelConfig) -> int:
        default_max_batch_size = model_config.max_batch_size()
        default_instance_count = model_config.instance_group_count(
            system_gpu_count=len(self._gpus)
        )
        default_concurrency = 2 * default_max_batch_size * default_instance_count

        return default_concurrency

    def _create_model_run_config(
        self,
        model: ModelProfileSpec,
        model_config_variant: ModelConfigVariant,
        concurrency: int,
    ) -> ModelRunConfig:
        perf_analyzer_config = self._create_perf_analyzer_config(
            model.model_name(), model, concurrency
        )
        model_run_config = ModelRunConfig(
            model.model_name(), model_config_variant, perf_analyzer_config
        )

        return model_run_config

    def _create_perf_analyzer_config(
        self,
        model_name: str,
        model: ModelProfileSpec,
        concurrency: int,
    ) -> PerfAnalyzerConfig:
        perf_analyzer_config = PerfAnalyzerConfig()

        perf_analyzer_config.update_config_from_profile_config(model_name, self._config)

        perf_config_params = {"batch-size": 1, "concurrency-range": concurrency}
        perf_analyzer_config.update_config(perf_config_params)

        perf_analyzer_config.update_config(model.perf_analyzer_flags())
        return perf_analyzer_config
